{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Intro.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMwgJMUcTr4b2AKq+Wipkhh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Happy-Virus-IkBeom/Pytorch/blob/master/Intro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNCGHiY58Hyw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "outputId": "1a54eac8-7146-47e5-c890-d024a0479aad"
      },
      "source": [
        "!pip3 install http://download.pytorch.org/whl/cu80/torch-0.3.0.post4-cp36-cp36m-linux_x86_64.whl\n",
        "!pip3 install torchvision"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torch==0.3.0.post4\n",
            "\u001b[?25l  Downloading http://download.pytorch.org/whl/cu80/torch-0.3.0.post4-cp36-cp36m-linux_x86_64.whl (592.3MB)\n",
            "\u001b[K     |████████████████████████████████| 592.3MB 1.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from torch==0.3.0.post4) (3.13)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==0.3.0.post4) (1.18.5)\n",
            "\u001b[31mERROR: torchvision 0.6.1+cu101 has requirement torch==1.5.1, but you'll have torch 0.3.0.post4 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: fastai 1.0.61 has requirement torch>=1.0.0, but you'll have torch 0.3.0.post4 which is incompatible.\u001b[0m\n",
            "Installing collected packages: torch\n",
            "  Found existing installation: torch 1.5.1+cu101\n",
            "    Uninstalling torch-1.5.1+cu101:\n",
            "      Successfully uninstalled torch-1.5.1+cu101\n",
            "Successfully installed torch-0.3.0.post4\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.6.1+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.18.5)\n",
            "Collecting torch==1.5.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/62/01/457b49d790b6c4b9720e6f9dbbb617692f6ce8afdaadf425c055c41a7416/torch-1.5.1-cp36-cp36m-manylinux1_x86_64.whl (753.2MB)\n",
            "\u001b[K     |████████████████████████████████| 753.2MB 20kB/s \n",
            "\u001b[?25hRequirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (7.0.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==1.5.1->torchvision) (0.16.0)\n",
            "Installing collected packages: torch\n",
            "  Found existing installation: torch 0.3.0.post4\n",
            "    Uninstalling torch-0.3.0.post4:\n",
            "      Successfully uninstalled torch-0.3.0.post4\n",
            "Successfully installed torch-1.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8oQ_6eF3UISC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXy5RQlagCdd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "cb52ed6d-958c-425a-9387-11551544684f"
      },
      "source": [
        "from torch import nn\n",
        "import torch\n",
        "from torch import tensor\n",
        "\n",
        "x_data = tensor([[1.0], [2.0], [3.0]])\n",
        "y_data = tensor([[2.0], [4.0], [6.0]])\n",
        "\n",
        "\n",
        "class Model(nn.Module): # Class Model은 항상 nn.Module을 상속받아야 함.\n",
        "    def __init__(self): # __init__ 의 처음 인자는 항상 self\n",
        "        \"\"\"\n",
        "        In the constructor we instantiate two nn.Linear module\n",
        "        \"\"\"\n",
        "        super(Model, self).__init__() # class형태의 모델은 항상 super(모델명,self).__init__()을 통해 nn.Module.__init__()을 실행시키는 코드가 필요함.\n",
        "        self.linear = torch.nn.Linear(1, 1)  # One in and one out\n",
        "\n",
        "    def forward(self, x): # forward propagation\n",
        "        \"\"\"\n",
        "        In the forward function we accept a Variable of input data and we must return\n",
        "        a Variable of output data. We can use Modules defined in the constructor as\n",
        "        well as arbitrary operators on Variables.\n",
        "        \"\"\"\n",
        "        y_pred = self.linear(x)\n",
        "        return y_pred\n",
        "\n",
        "\n",
        "# our model\n",
        "model = Model()\n",
        "\n",
        "# Construct our loss function and an Optimizer. The call to model.parameters()\n",
        "# in the SGD constructor will contain the learnable parameters of the two\n",
        "# nn.Linear modules which are members of the model.\n",
        "criterion = torch.nn.MSELoss(reduction='sum')\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(500):\n",
        "    # 1) Forward pass: Compute predicted y by passing x to the model\n",
        "    y_pred = model(x_data)\n",
        "\n",
        "    # 2) Compute and print loss\n",
        "    loss = criterion(y_pred, y_data)\n",
        "    print(f'Epoch: {epoch} | Loss: {loss.item()} ')\n",
        "\n",
        "    # Zero gradients, perform a backward pass, and update the weights.\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "# After training\n",
        "hour_var = tensor([[4.0]])\n",
        "y_pred = model(hour_var)\n",
        "print(\"Prediction (after training)\",  4, model(hour_var).data[0][0].item())"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0 | Loss: 92.18265533447266 \n",
            "Epoch: 1 | Loss: 41.11121368408203 \n",
            "Epoch: 2 | Loss: 18.37459373474121 \n",
            "Epoch: 3 | Loss: 8.251846313476562 \n",
            "Epoch: 4 | Loss: 3.7444534301757812 \n",
            "Epoch: 5 | Loss: 1.7368676662445068 \n",
            "Epoch: 6 | Loss: 0.8421447277069092 \n",
            "Epoch: 7 | Loss: 0.4428480565547943 \n",
            "Epoch: 8 | Loss: 0.264115571975708 \n",
            "Epoch: 9 | Loss: 0.18358656764030457 \n",
            "Epoch: 10 | Loss: 0.14678871631622314 \n",
            "Epoch: 11 | Loss: 0.12947222590446472 \n",
            "Epoch: 12 | Loss: 0.12084193527698517 \n",
            "Epoch: 13 | Loss: 0.11609166860580444 \n",
            "Epoch: 14 | Loss: 0.11308154463768005 \n",
            "Epoch: 15 | Loss: 0.11085933446884155 \n",
            "Epoch: 16 | Loss: 0.10900023579597473 \n",
            "Epoch: 17 | Loss: 0.10731533169746399 \n",
            "Epoch: 18 | Loss: 0.10572031140327454 \n",
            "Epoch: 19 | Loss: 0.10417749732732773 \n",
            "Epoch: 20 | Loss: 0.10266990959644318 \n",
            "Epoch: 21 | Loss: 0.1011897623538971 \n",
            "Epoch: 22 | Loss: 0.09973321855068207 \n",
            "Epoch: 23 | Loss: 0.09829919785261154 \n",
            "Epoch: 24 | Loss: 0.09688600897789001 \n",
            "Epoch: 25 | Loss: 0.09549335390329361 \n",
            "Epoch: 26 | Loss: 0.09412091970443726 \n",
            "Epoch: 27 | Loss: 0.09276822954416275 \n",
            "Epoch: 28 | Loss: 0.09143498539924622 \n",
            "Epoch: 29 | Loss: 0.09012089669704437 \n",
            "Epoch: 30 | Loss: 0.08882586658000946 \n",
            "Epoch: 31 | Loss: 0.08754904568195343 \n",
            "Epoch: 32 | Loss: 0.0862908586859703 \n",
            "Epoch: 33 | Loss: 0.0850507915019989 \n",
            "Epoch: 34 | Loss: 0.08382842689752579 \n",
            "Epoch: 35 | Loss: 0.08262379467487335 \n",
            "Epoch: 36 | Loss: 0.08143617957830429 \n",
            "Epoch: 37 | Loss: 0.08026589453220367 \n",
            "Epoch: 38 | Loss: 0.07911235094070435 \n",
            "Epoch: 39 | Loss: 0.07797543704509735 \n",
            "Epoch: 40 | Loss: 0.07685475051403046 \n",
            "Epoch: 41 | Loss: 0.07575012743473053 \n",
            "Epoch: 42 | Loss: 0.07466151565313339 \n",
            "Epoch: 43 | Loss: 0.07358858734369278 \n",
            "Epoch: 44 | Loss: 0.07253103703260422 \n",
            "Epoch: 45 | Loss: 0.07148865610361099 \n",
            "Epoch: 46 | Loss: 0.07046125084161758 \n",
            "Epoch: 47 | Loss: 0.0694485679268837 \n",
            "Epoch: 48 | Loss: 0.06845050305128098 \n",
            "Epoch: 49 | Loss: 0.06746677309274673 \n",
            "Epoch: 50 | Loss: 0.06649721413850784 \n",
            "Epoch: 51 | Loss: 0.06554144620895386 \n",
            "Epoch: 52 | Loss: 0.06459946930408478 \n",
            "Epoch: 53 | Loss: 0.06367123126983643 \n",
            "Epoch: 54 | Loss: 0.06275607645511627 \n",
            "Epoch: 55 | Loss: 0.061854053288698196 \n",
            "Epoch: 56 | Loss: 0.06096523627638817 \n",
            "Epoch: 57 | Loss: 0.06008914113044739 \n",
            "Epoch: 58 | Loss: 0.05922560393810272 \n",
            "Epoch: 59 | Loss: 0.05837428197264671 \n",
            "Epoch: 60 | Loss: 0.057535476982593536 \n",
            "Epoch: 61 | Loss: 0.05670859292149544 \n",
            "Epoch: 62 | Loss: 0.05589359626173973 \n",
            "Epoch: 63 | Loss: 0.05509023368358612 \n",
            "Epoch: 64 | Loss: 0.05429847165942192 \n",
            "Epoch: 65 | Loss: 0.053518131375312805 \n",
            "Epoch: 66 | Loss: 0.05274906009435654 \n",
            "Epoch: 67 | Loss: 0.05199097841978073 \n",
            "Epoch: 68 | Loss: 0.05124379321932793 \n",
            "Epoch: 69 | Loss: 0.050507307052612305 \n",
            "Epoch: 70 | Loss: 0.04978136718273163 \n",
            "Epoch: 71 | Loss: 0.049066003412008286 \n",
            "Epoch: 72 | Loss: 0.04836076498031616 \n",
            "Epoch: 73 | Loss: 0.04766576737165451 \n",
            "Epoch: 74 | Loss: 0.04698078706860542 \n",
            "Epoch: 75 | Loss: 0.04630555212497711 \n",
            "Epoch: 76 | Loss: 0.04564007371664047 \n",
            "Epoch: 77 | Loss: 0.04498424008488655 \n",
            "Epoch: 78 | Loss: 0.04433770477771759 \n",
            "Epoch: 79 | Loss: 0.043700508773326874 \n",
            "Epoch: 80 | Loss: 0.04307239502668381 \n",
            "Epoch: 81 | Loss: 0.042453426867723465 \n",
            "Epoch: 82 | Loss: 0.041843365877866745 \n",
            "Epoch: 83 | Loss: 0.04124199599027634 \n",
            "Epoch: 84 | Loss: 0.04064931720495224 \n",
            "Epoch: 85 | Loss: 0.04006510600447655 \n",
            "Epoch: 86 | Loss: 0.03948938101530075 \n",
            "Epoch: 87 | Loss: 0.03892172500491142 \n",
            "Epoch: 88 | Loss: 0.038362354040145874 \n",
            "Epoch: 89 | Loss: 0.0378110408782959 \n",
            "Epoch: 90 | Loss: 0.03726763278245926 \n",
            "Epoch: 91 | Loss: 0.03673206642270088 \n",
            "Epoch: 92 | Loss: 0.03620419651269913 \n",
            "Epoch: 93 | Loss: 0.03568383678793907 \n",
            "Epoch: 94 | Loss: 0.035170961171388626 \n",
            "Epoch: 95 | Loss: 0.03466556593775749 \n",
            "Epoch: 96 | Loss: 0.0341673418879509 \n",
            "Epoch: 97 | Loss: 0.03367628902196884 \n",
            "Epoch: 98 | Loss: 0.033192358911037445 \n",
            "Epoch: 99 | Loss: 0.03271535038948059 \n",
            "Epoch: 100 | Loss: 0.032245125621557236 \n",
            "Epoch: 101 | Loss: 0.03178170323371887 \n",
            "Epoch: 102 | Loss: 0.031324949115514755 \n",
            "Epoch: 103 | Loss: 0.030874721705913544 \n",
            "Epoch: 104 | Loss: 0.03043106570839882 \n",
            "Epoch: 105 | Loss: 0.0299936942756176 \n",
            "Epoch: 106 | Loss: 0.02956259809434414 \n",
            "Epoch: 107 | Loss: 0.02913776785135269 \n",
            "Epoch: 108 | Loss: 0.028719058260321617 \n",
            "Epoch: 109 | Loss: 0.028306273743510246 \n",
            "Epoch: 110 | Loss: 0.027899477630853653 \n",
            "Epoch: 111 | Loss: 0.027498500421643257 \n",
            "Epoch: 112 | Loss: 0.027103226631879807 \n",
            "Epoch: 113 | Loss: 0.026713840663433075 \n",
            "Epoch: 114 | Loss: 0.026329923421144485 \n",
            "Epoch: 115 | Loss: 0.025951482355594635 \n",
            "Epoch: 116 | Loss: 0.02557854913175106 \n",
            "Epoch: 117 | Loss: 0.025210890918970108 \n",
            "Epoch: 118 | Loss: 0.024848556146025658 \n",
            "Epoch: 119 | Loss: 0.024491453543305397 \n",
            "Epoch: 120 | Loss: 0.024139471352100372 \n",
            "Epoch: 121 | Loss: 0.023792577907443047 \n",
            "Epoch: 122 | Loss: 0.023450616747140884 \n",
            "Epoch: 123 | Loss: 0.023113613948225975 \n",
            "Epoch: 124 | Loss: 0.022781427949666977 \n",
            "Epoch: 125 | Loss: 0.02245406061410904 \n",
            "Epoch: 126 | Loss: 0.022131310775876045 \n",
            "Epoch: 127 | Loss: 0.02181328646838665 \n",
            "Epoch: 128 | Loss: 0.021499771624803543 \n",
            "Epoch: 129 | Loss: 0.021190818399190903 \n",
            "Epoch: 130 | Loss: 0.020886220037937164 \n",
            "Epoch: 131 | Loss: 0.02058609575033188 \n",
            "Epoch: 132 | Loss: 0.020290229469537735 \n",
            "Epoch: 133 | Loss: 0.01999867707490921 \n",
            "Epoch: 134 | Loss: 0.019711224362254143 \n",
            "Epoch: 135 | Loss: 0.019427984952926636 \n",
            "Epoch: 136 | Loss: 0.01914869248867035 \n",
            "Epoch: 137 | Loss: 0.018873542547225952 \n",
            "Epoch: 138 | Loss: 0.01860232464969158 \n",
            "Epoch: 139 | Loss: 0.018334897235035896 \n",
            "Epoch: 140 | Loss: 0.018071452155709267 \n",
            "Epoch: 141 | Loss: 0.017811786383390427 \n",
            "Epoch: 142 | Loss: 0.017555762082338333 \n",
            "Epoch: 143 | Loss: 0.017303425818681717 \n",
            "Epoch: 144 | Loss: 0.017054777592420578 \n",
            "Epoch: 145 | Loss: 0.01680968515574932 \n",
            "Epoch: 146 | Loss: 0.01656804420053959 \n",
            "Epoch: 147 | Loss: 0.016329942271113396 \n",
            "Epoch: 148 | Loss: 0.01609526015818119 \n",
            "Epoch: 149 | Loss: 0.01586391031742096 \n",
            "Epoch: 150 | Loss: 0.015635965391993523 \n",
            "Epoch: 151 | Loss: 0.01541125774383545 \n",
            "Epoch: 152 | Loss: 0.015189776197075844 \n",
            "Epoch: 153 | Loss: 0.014971496537327766 \n",
            "Epoch: 154 | Loss: 0.014756322838366032 \n",
            "Epoch: 155 | Loss: 0.014544269070029259 \n",
            "Epoch: 156 | Loss: 0.01433522067964077 \n",
            "Epoch: 157 | Loss: 0.014129171147942543 \n",
            "Epoch: 158 | Loss: 0.013926169835031033 \n",
            "Epoch: 159 | Loss: 0.013726000674068928 \n",
            "Epoch: 160 | Loss: 0.0135287344455719 \n",
            "Epoch: 161 | Loss: 0.013334320858120918 \n",
            "Epoch: 162 | Loss: 0.013142632320523262 \n",
            "Epoch: 163 | Loss: 0.012953794561326504 \n",
            "Epoch: 164 | Loss: 0.012767643667757511 \n",
            "Epoch: 165 | Loss: 0.012584150768816471 \n",
            "Epoch: 166 | Loss: 0.012403287924826145 \n",
            "Epoch: 167 | Loss: 0.012225027196109295 \n",
            "Epoch: 168 | Loss: 0.012049363926053047 \n",
            "Epoch: 169 | Loss: 0.011876145377755165 \n",
            "Epoch: 170 | Loss: 0.011705494485795498 \n",
            "Epoch: 171 | Loss: 0.01153729110956192 \n",
            "Epoch: 172 | Loss: 0.011371464468538761 \n",
            "Epoch: 173 | Loss: 0.011208037845790386 \n",
            "Epoch: 174 | Loss: 0.011046968400478363 \n",
            "Epoch: 175 | Loss: 0.010888167656958103 \n",
            "Epoch: 176 | Loss: 0.010731680318713188 \n",
            "Epoch: 177 | Loss: 0.010577497072517872 \n",
            "Epoch: 178 | Loss: 0.010425485670566559 \n",
            "Epoch: 179 | Loss: 0.01027563028037548 \n",
            "Epoch: 180 | Loss: 0.010127954185009003 \n",
            "Epoch: 181 | Loss: 0.009982386603951454 \n",
            "Epoch: 182 | Loss: 0.009838905185461044 \n",
            "Epoch: 183 | Loss: 0.009697492234408855 \n",
            "Epoch: 184 | Loss: 0.009558139368891716 \n",
            "Epoch: 185 | Loss: 0.009420792572200298 \n",
            "Epoch: 186 | Loss: 0.00928538665175438 \n",
            "Epoch: 187 | Loss: 0.009151972830295563 \n",
            "Epoch: 188 | Loss: 0.00902039185166359 \n",
            "Epoch: 189 | Loss: 0.0088907890021801 \n",
            "Epoch: 190 | Loss: 0.008763022720813751 \n",
            "Epoch: 191 | Loss: 0.008637098595499992 \n",
            "Epoch: 192 | Loss: 0.00851293932646513 \n",
            "Epoch: 193 | Loss: 0.008390584029257298 \n",
            "Epoch: 194 | Loss: 0.008269988000392914 \n",
            "Epoch: 195 | Loss: 0.008151208981871605 \n",
            "Epoch: 196 | Loss: 0.008033987134695053 \n",
            "Epoch: 197 | Loss: 0.007918545976281166 \n",
            "Epoch: 198 | Loss: 0.007804793305695057 \n",
            "Epoch: 199 | Loss: 0.007692612707614899 \n",
            "Epoch: 200 | Loss: 0.007582039572298527 \n",
            "Epoch: 201 | Loss: 0.007473045494407415 \n",
            "Epoch: 202 | Loss: 0.0073656789027154446 \n",
            "Epoch: 203 | Loss: 0.0072598229162395 \n",
            "Epoch: 204 | Loss: 0.007155465427786112 \n",
            "Epoch: 205 | Loss: 0.0070526255294680595 \n",
            "Epoch: 206 | Loss: 0.006951271556317806 \n",
            "Epoch: 207 | Loss: 0.00685142120346427 \n",
            "Epoch: 208 | Loss: 0.006752914283424616 \n",
            "Epoch: 209 | Loss: 0.006655832752585411 \n",
            "Epoch: 210 | Loss: 0.006560211535543203 \n",
            "Epoch: 211 | Loss: 0.006465922575443983 \n",
            "Epoch: 212 | Loss: 0.0063730087131261826 \n",
            "Epoch: 213 | Loss: 0.0062814210541546345 \n",
            "Epoch: 214 | Loss: 0.006191154941916466 \n",
            "Epoch: 215 | Loss: 0.0061021787114441395 \n",
            "Epoch: 216 | Loss: 0.0060144453309476376 \n",
            "Epoch: 217 | Loss: 0.0059280237182974815 \n",
            "Epoch: 218 | Loss: 0.005842863116413355 \n",
            "Epoch: 219 | Loss: 0.005758870393037796 \n",
            "Epoch: 220 | Loss: 0.0056761023588478565 \n",
            "Epoch: 221 | Loss: 0.0055945259518921375 \n",
            "Epoch: 222 | Loss: 0.00551412720233202 \n",
            "Epoch: 223 | Loss: 0.005434888415038586 \n",
            "Epoch: 224 | Loss: 0.005356780719012022 \n",
            "Epoch: 225 | Loss: 0.005279792007058859 \n",
            "Epoch: 226 | Loss: 0.00520387664437294 \n",
            "Epoch: 227 | Loss: 0.005129110533744097 \n",
            "Epoch: 228 | Loss: 0.005055414512753487 \n",
            "Epoch: 229 | Loss: 0.00498274015262723 \n",
            "Epoch: 230 | Loss: 0.004911134019494057 \n",
            "Epoch: 231 | Loss: 0.004840563982725143 \n",
            "Epoch: 232 | Loss: 0.004770982079207897 \n",
            "Epoch: 233 | Loss: 0.004702450707554817 \n",
            "Epoch: 234 | Loss: 0.004634850192815065 \n",
            "Epoch: 235 | Loss: 0.004568235017359257 \n",
            "Epoch: 236 | Loss: 0.004502573050558567 \n",
            "Epoch: 237 | Loss: 0.004437878727912903 \n",
            "Epoch: 238 | Loss: 0.004374079871922731 \n",
            "Epoch: 239 | Loss: 0.004311234690248966 \n",
            "Epoch: 240 | Loss: 0.004249265417456627 \n",
            "Epoch: 241 | Loss: 0.0041882190853357315 \n",
            "Epoch: 242 | Loss: 0.0041280100122094154 \n",
            "Epoch: 243 | Loss: 0.004068681970238686 \n",
            "Epoch: 244 | Loss: 0.004010200034826994 \n",
            "Epoch: 245 | Loss: 0.003952564671635628 \n",
            "Epoch: 246 | Loss: 0.0038957931101322174 \n",
            "Epoch: 247 | Loss: 0.0038398033939301968 \n",
            "Epoch: 248 | Loss: 0.003784587373957038 \n",
            "Epoch: 249 | Loss: 0.003730225842446089 \n",
            "Epoch: 250 | Loss: 0.0036766312550753355 \n",
            "Epoch: 251 | Loss: 0.003623743075877428 \n",
            "Epoch: 252 | Loss: 0.0035716723650693893 \n",
            "Epoch: 253 | Loss: 0.0035203611478209496 \n",
            "Epoch: 254 | Loss: 0.0034697537776082754 \n",
            "Epoch: 255 | Loss: 0.0034198728390038013 \n",
            "Epoch: 256 | Loss: 0.003370731370523572 \n",
            "Epoch: 257 | Loss: 0.0033222977072000504 \n",
            "Epoch: 258 | Loss: 0.003274556715041399 \n",
            "Epoch: 259 | Loss: 0.0032274918630719185 \n",
            "Epoch: 260 | Loss: 0.0031811087392270565 \n",
            "Epoch: 261 | Loss: 0.003135399892926216 \n",
            "Epoch: 262 | Loss: 0.0030903287697583437 \n",
            "Epoch: 263 | Loss: 0.003045924473553896 \n",
            "Epoch: 264 | Loss: 0.003002127865329385 \n",
            "Epoch: 265 | Loss: 0.00295900902710855 \n",
            "Epoch: 266 | Loss: 0.002916469005867839 \n",
            "Epoch: 267 | Loss: 0.0028745767194777727 \n",
            "Epoch: 268 | Loss: 0.002833232283592224 \n",
            "Epoch: 269 | Loss: 0.002792519051581621 \n",
            "Epoch: 270 | Loss: 0.0027524083852767944 \n",
            "Epoch: 271 | Loss: 0.0027128453366458416 \n",
            "Epoch: 272 | Loss: 0.002673860639333725 \n",
            "Epoch: 273 | Loss: 0.002635447308421135 \n",
            "Epoch: 274 | Loss: 0.0025975462049245834 \n",
            "Epoch: 275 | Loss: 0.0025602399837225676 \n",
            "Epoch: 276 | Loss: 0.0025234376080334187 \n",
            "Epoch: 277 | Loss: 0.0024871567729860544 \n",
            "Epoch: 278 | Loss: 0.002451426349580288 \n",
            "Epoch: 279 | Loss: 0.002416186034679413 \n",
            "Epoch: 280 | Loss: 0.0023814565502107143 \n",
            "Epoch: 281 | Loss: 0.002347248373553157 \n",
            "Epoch: 282 | Loss: 0.0023134867660701275 \n",
            "Epoch: 283 | Loss: 0.0022802469320595264 \n",
            "Epoch: 284 | Loss: 0.002247462747618556 \n",
            "Epoch: 285 | Loss: 0.00221517332829535 \n",
            "Epoch: 286 | Loss: 0.002183329313993454 \n",
            "Epoch: 287 | Loss: 0.0021519693545997143 \n",
            "Epoch: 288 | Loss: 0.0021210452541708946 \n",
            "Epoch: 289 | Loss: 0.0020905574783682823 \n",
            "Epoch: 290 | Loss: 0.0020605335012078285 \n",
            "Epoch: 291 | Loss: 0.002030894160270691 \n",
            "Epoch: 292 | Loss: 0.002001706510782242 \n",
            "Epoch: 293 | Loss: 0.0019729440100491047 \n",
            "Epoch: 294 | Loss: 0.0019446003716439009 \n",
            "Epoch: 295 | Loss: 0.0019166499841958284 \n",
            "Epoch: 296 | Loss: 0.0018891142681241035 \n",
            "Epoch: 297 | Loss: 0.0018619582988321781 \n",
            "Epoch: 298 | Loss: 0.001835190923884511 \n",
            "Epoch: 299 | Loss: 0.0018088150536641479 \n",
            "Epoch: 300 | Loss: 0.0017828167183324695 \n",
            "Epoch: 301 | Loss: 0.0017571868374943733 \n",
            "Epoch: 302 | Loss: 0.0017319460166618228 \n",
            "Epoch: 303 | Loss: 0.0017070414032787085 \n",
            "Epoch: 304 | Loss: 0.001682522241026163 \n",
            "Epoch: 305 | Loss: 0.0016583319520577788 \n",
            "Epoch: 306 | Loss: 0.0016344955656677485 \n",
            "Epoch: 307 | Loss: 0.0016110067954286933 \n",
            "Epoch: 308 | Loss: 0.0015878640115261078 \n",
            "Epoch: 309 | Loss: 0.0015650589484721422 \n",
            "Epoch: 310 | Loss: 0.0015425510937348008 \n",
            "Epoch: 311 | Loss: 0.001520394696854055 \n",
            "Epoch: 312 | Loss: 0.0014985335292294621 \n",
            "Epoch: 313 | Loss: 0.0014770005363970995 \n",
            "Epoch: 314 | Loss: 0.0014557675458490849 \n",
            "Epoch: 315 | Loss: 0.0014348600525408983 \n",
            "Epoch: 316 | Loss: 0.0014142283471301198 \n",
            "Epoch: 317 | Loss: 0.001393889426253736 \n",
            "Epoch: 318 | Loss: 0.0013738659908995032 \n",
            "Epoch: 319 | Loss: 0.0013541156658902764 \n",
            "Epoch: 320 | Loss: 0.0013346634805202484 \n",
            "Epoch: 321 | Loss: 0.0013154953485354781 \n",
            "Epoch: 322 | Loss: 0.001296587404794991 \n",
            "Epoch: 323 | Loss: 0.001277941046282649 \n",
            "Epoch: 324 | Loss: 0.0012595700100064278 \n",
            "Epoch: 325 | Loss: 0.0012414834927767515 \n",
            "Epoch: 326 | Loss: 0.0012236235197633505 \n",
            "Epoch: 327 | Loss: 0.001206040265969932 \n",
            "Epoch: 328 | Loss: 0.001188722555525601 \n",
            "Epoch: 329 | Loss: 0.0011716337176039815 \n",
            "Epoch: 330 | Loss: 0.0011547791073098779 \n",
            "Epoch: 331 | Loss: 0.0011382015654817224 \n",
            "Epoch: 332 | Loss: 0.001121829729527235 \n",
            "Epoch: 333 | Loss: 0.001105719362385571 \n",
            "Epoch: 334 | Loss: 0.001089825527742505 \n",
            "Epoch: 335 | Loss: 0.0010741590522229671 \n",
            "Epoch: 336 | Loss: 0.0010587252909317613 \n",
            "Epoch: 337 | Loss: 0.0010434932773932815 \n",
            "Epoch: 338 | Loss: 0.0010285190073773265 \n",
            "Epoch: 339 | Loss: 0.0010137306526303291 \n",
            "Epoch: 340 | Loss: 0.000999161507934332 \n",
            "Epoch: 341 | Loss: 0.0009848055196925998 \n",
            "Epoch: 342 | Loss: 0.0009706475539132953 \n",
            "Epoch: 343 | Loss: 0.0009567035594955087 \n",
            "Epoch: 344 | Loss: 0.0009429623023606837 \n",
            "Epoch: 345 | Loss: 0.0009294020128436387 \n",
            "Epoch: 346 | Loss: 0.0009160349145531654 \n",
            "Epoch: 347 | Loss: 0.0009028642671182752 \n",
            "Epoch: 348 | Loss: 0.0008898929809220135 \n",
            "Epoch: 349 | Loss: 0.0008771103457547724 \n",
            "Epoch: 350 | Loss: 0.000864502158947289 \n",
            "Epoch: 351 | Loss: 0.0008520803530700505 \n",
            "Epoch: 352 | Loss: 0.0008398343925364316 \n",
            "Epoch: 353 | Loss: 0.0008277673623524606 \n",
            "Epoch: 354 | Loss: 0.0008158573764376342 \n",
            "Epoch: 355 | Loss: 0.0008041343535296619 \n",
            "Epoch: 356 | Loss: 0.000792581238783896 \n",
            "Epoch: 357 | Loss: 0.00078118487726897 \n",
            "Epoch: 358 | Loss: 0.0007699694251641631 \n",
            "Epoch: 359 | Loss: 0.0007588927401229739 \n",
            "Epoch: 360 | Loss: 0.0007479949272237718 \n",
            "Epoch: 361 | Loss: 0.000737242226023227 \n",
            "Epoch: 362 | Loss: 0.0007266519824042916 \n",
            "Epoch: 363 | Loss: 0.0007162067922763526 \n",
            "Epoch: 364 | Loss: 0.0007059068302623928 \n",
            "Epoch: 365 | Loss: 0.0006957596633583307 \n",
            "Epoch: 366 | Loss: 0.0006857680855318904 \n",
            "Epoch: 367 | Loss: 0.0006759259849786758 \n",
            "Epoch: 368 | Loss: 0.000666204490698874 \n",
            "Epoch: 369 | Loss: 0.0006566264200955629 \n",
            "Epoch: 370 | Loss: 0.0006471857777796686 \n",
            "Epoch: 371 | Loss: 0.0006378809921443462 \n",
            "Epoch: 372 | Loss: 0.0006287252763286233 \n",
            "Epoch: 373 | Loss: 0.0006196849280968308 \n",
            "Epoch: 374 | Loss: 0.0006107833469286561 \n",
            "Epoch: 375 | Loss: 0.000601988984271884 \n",
            "Epoch: 376 | Loss: 0.0005933532956987619 \n",
            "Epoch: 377 | Loss: 0.0005848206928931177 \n",
            "Epoch: 378 | Loss: 0.0005764239467680454 \n",
            "Epoch: 379 | Loss: 0.0005681377369910479 \n",
            "Epoch: 380 | Loss: 0.0005599652649834752 \n",
            "Epoch: 381 | Loss: 0.0005519257392734289 \n",
            "Epoch: 382 | Loss: 0.0005439918022602797 \n",
            "Epoch: 383 | Loss: 0.000536175852175802 \n",
            "Epoch: 384 | Loss: 0.0005284659564495087 \n",
            "Epoch: 385 | Loss: 0.0005208821967244148 \n",
            "Epoch: 386 | Loss: 0.0005133835365995765 \n",
            "Epoch: 387 | Loss: 0.000506006006617099 \n",
            "Epoch: 388 | Loss: 0.000498732493724674 \n",
            "Epoch: 389 | Loss: 0.0004915773752145469 \n",
            "Epoch: 390 | Loss: 0.00048449443420395255 \n",
            "Epoch: 391 | Loss: 0.0004775378620252013 \n",
            "Epoch: 392 | Loss: 0.00047066816478036344 \n",
            "Epoch: 393 | Loss: 0.0004639058606699109 \n",
            "Epoch: 394 | Loss: 0.00045725092059001327 \n",
            "Epoch: 395 | Loss: 0.00045066626626066864 \n",
            "Epoch: 396 | Loss: 0.00044419552432373166 \n",
            "Epoch: 397 | Loss: 0.0004378190787974745 \n",
            "Epoch: 398 | Loss: 0.0004315237165428698 \n",
            "Epoch: 399 | Loss: 0.00042530993232503533 \n",
            "Epoch: 400 | Loss: 0.00041921023512259126 \n",
            "Epoch: 401 | Loss: 0.0004131841706112027 \n",
            "Epoch: 402 | Loss: 0.0004072427691426128 \n",
            "Epoch: 403 | Loss: 0.00040138870826922357 \n",
            "Epoch: 404 | Loss: 0.0003956221044063568 \n",
            "Epoch: 405 | Loss: 0.00038993050111457705 \n",
            "Epoch: 406 | Loss: 0.00038433034205809236 \n",
            "Epoch: 407 | Loss: 0.00037881138268858194 \n",
            "Epoch: 408 | Loss: 0.00037337056710384786 \n",
            "Epoch: 409 | Loss: 0.0003680070221889764 \n",
            "Epoch: 410 | Loss: 0.00036270887358114123 \n",
            "Epoch: 411 | Loss: 0.0003574974834918976 \n",
            "Epoch: 412 | Loss: 0.00035236639087088406 \n",
            "Epoch: 413 | Loss: 0.0003472951066214591 \n",
            "Epoch: 414 | Loss: 0.0003422993468120694 \n",
            "Epoch: 415 | Loss: 0.0003373910440132022 \n",
            "Epoch: 416 | Loss: 0.00033253192668780684 \n",
            "Epoch: 417 | Loss: 0.0003277544165030122 \n",
            "Epoch: 418 | Loss: 0.00032304387423209846 \n",
            "Epoch: 419 | Loss: 0.000318405800499022 \n",
            "Epoch: 420 | Loss: 0.0003138259635306895 \n",
            "Epoch: 421 | Loss: 0.00030931615037843585 \n",
            "Epoch: 422 | Loss: 0.0003048694343306124 \n",
            "Epoch: 423 | Loss: 0.00030048610642552376 \n",
            "Epoch: 424 | Loss: 0.0002961686113849282 \n",
            "Epoch: 425 | Loss: 0.0002919160760939121 \n",
            "Epoch: 426 | Loss: 0.00028771997313015163 \n",
            "Epoch: 427 | Loss: 0.0002835855702869594 \n",
            "Epoch: 428 | Loss: 0.00027951219817623496 \n",
            "Epoch: 429 | Loss: 0.00027549336664378643 \n",
            "Epoch: 430 | Loss: 0.0002715362352319062 \n",
            "Epoch: 431 | Loss: 0.0002676334115676582 \n",
            "Epoch: 432 | Loss: 0.0002637786674313247 \n",
            "Epoch: 433 | Loss: 0.00025999126955866814 \n",
            "Epoch: 434 | Loss: 0.0002562554436735809 \n",
            "Epoch: 435 | Loss: 0.00025257436209358275 \n",
            "Epoch: 436 | Loss: 0.00024894834496080875 \n",
            "Epoch: 437 | Loss: 0.0002453640627209097 \n",
            "Epoch: 438 | Loss: 0.00024184281937777996 \n",
            "Epoch: 439 | Loss: 0.0002383623068453744 \n",
            "Epoch: 440 | Loss: 0.0002349399874219671 \n",
            "Epoch: 441 | Loss: 0.00023155657981988043 \n",
            "Epoch: 442 | Loss: 0.0002282345958519727 \n",
            "Epoch: 443 | Loss: 0.00022495318262372166 \n",
            "Epoch: 444 | Loss: 0.00022171804448589683 \n",
            "Epoch: 445 | Loss: 0.00021853383805137128 \n",
            "Epoch: 446 | Loss: 0.00021539145382121205 \n",
            "Epoch: 447 | Loss: 0.00021230314450804144 \n",
            "Epoch: 448 | Loss: 0.00020925061835441738 \n",
            "Epoch: 449 | Loss: 0.00020623854652512819 \n",
            "Epoch: 450 | Loss: 0.00020327237143646926 \n",
            "Epoch: 451 | Loss: 0.00020036057685501873 \n",
            "Epoch: 452 | Loss: 0.00019747225451283157 \n",
            "Epoch: 453 | Loss: 0.00019463735225144774 \n",
            "Epoch: 454 | Loss: 0.00019184227858204395 \n",
            "Epoch: 455 | Loss: 0.00018909058417193592 \n",
            "Epoch: 456 | Loss: 0.00018636989989317954 \n",
            "Epoch: 457 | Loss: 0.00018369256576988846 \n",
            "Epoch: 458 | Loss: 0.00018104477203451097 \n",
            "Epoch: 459 | Loss: 0.00017844728427007794 \n",
            "Epoch: 460 | Loss: 0.0001758793368935585 \n",
            "Epoch: 461 | Loss: 0.00017335297889076173 \n",
            "Epoch: 462 | Loss: 0.00017086314619518816 \n",
            "Epoch: 463 | Loss: 0.00016840564785525203 \n",
            "Epoch: 464 | Loss: 0.00016598543152213097 \n",
            "Epoch: 465 | Loss: 0.00016359833534806967 \n",
            "Epoch: 466 | Loss: 0.0001612492196727544 \n",
            "Epoch: 467 | Loss: 0.00015893540694378316 \n",
            "Epoch: 468 | Loss: 0.00015665360842831433 \n",
            "Epoch: 469 | Loss: 0.0001543948019389063 \n",
            "Epoch: 470 | Loss: 0.00015217751206364483 \n",
            "Epoch: 471 | Loss: 0.0001499940553912893 \n",
            "Epoch: 472 | Loss: 0.00014783556980546564 \n",
            "Epoch: 473 | Loss: 0.00014571378414984792 \n",
            "Epoch: 474 | Loss: 0.0001436191814718768 \n",
            "Epoch: 475 | Loss: 0.00014154943346511573 \n",
            "Epoch: 476 | Loss: 0.00013952353037893772 \n",
            "Epoch: 477 | Loss: 0.00013751498772762716 \n",
            "Epoch: 478 | Loss: 0.00013553047028835863 \n",
            "Epoch: 479 | Loss: 0.00013358858996070921 \n",
            "Epoch: 480 | Loss: 0.00013166741700842977 \n",
            "Epoch: 481 | Loss: 0.00012977933511137962 \n",
            "Epoch: 482 | Loss: 0.00012791149492841214 \n",
            "Epoch: 483 | Loss: 0.00012606893142219633 \n",
            "Epoch: 484 | Loss: 0.00012426113244146109 \n",
            "Epoch: 485 | Loss: 0.00012247537961229682 \n",
            "Epoch: 486 | Loss: 0.00012071151286363602 \n",
            "Epoch: 487 | Loss: 0.00011898073717020452 \n",
            "Epoch: 488 | Loss: 0.00011727189121302217 \n",
            "Epoch: 489 | Loss: 0.0001155835489043966 \n",
            "Epoch: 490 | Loss: 0.0001139217201853171 \n",
            "Epoch: 491 | Loss: 0.00011228238145122305 \n",
            "Epoch: 492 | Loss: 0.0001106757263187319 \n",
            "Epoch: 493 | Loss: 0.00010908186959568411 \n",
            "Epoch: 494 | Loss: 0.00010751283844001591 \n",
            "Epoch: 495 | Loss: 0.00010596953507047147 \n",
            "Epoch: 496 | Loss: 0.00010444452345836908 \n",
            "Epoch: 497 | Loss: 0.00010294411913491786 \n",
            "Epoch: 498 | Loss: 0.0001014698063954711 \n",
            "Epoch: 499 | Loss: 0.00010000845941249281 \n",
            "Prediction (after training) 4 7.988503932952881\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0RJ-i6HRaz_g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn # Deep learning model에 필요한 모듈이 모아져 있는 페키지. ex) nn.Linear(128,128),nn.ReLU()\n",
        "import torch.nn.functional as F # nn과 같은 모듈이 모아져 있지만 함수의 input으로 반드시 연산이 되어야 하는 값을 받는다. ex) F.linear(X,128,128), R.relu(X)\n",
        "import torch.optim as optim # 학습에 관련된 optimizing method가 있는 패키지\n",
        "import torch.utils.data as data_utils # batch generator등 학습 데이터에 관련된 패키지\n",
        "\n",
        "class MyModel(nn.Module):\n",
        "\n",
        "  def __init__(self, X_dim, y_dim):\n",
        "    super(MyModel,self).__init__()\n",
        "    layer1 = nn.Linear(X_dim,128)\n",
        "    activation1 = nn.ReLU()\n",
        "    layer2 = nn.Linear(128,y_dim)\n",
        "    self.module = nn.Sequential(\n",
        "        layer1,\n",
        "        activation1,\n",
        "        layer2\n",
        "    )\n",
        "  \n",
        "  def forward(self,x):\n",
        "    out = self.module(x)\n",
        "    result = F.softmax(out, dim=1)\n",
        "    return result\n",
        "\n",
        "#준비재료\n",
        "\n",
        "criterion = nn.CrossEntropyLoss() #어떤 loss를 쓸것인지\n",
        "learning_rate = 1e-5              #learning rate\n",
        "optimizer = optim.SGD(model.parameters(),lr = learning_rate) # optimizer 무엇으로 할건지\n",
        "num_epochs = 2                    # 학습 횟수는 몇번으로 할것인지\n",
        "num_batches = len(train_loader)  \n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  for i , data in enumerate(train_loader): # 자료형(리스트, 튜플, 문자열)을 입력으로 받아 순서와 값을 전달.\n",
        "    x, x_labels = data # x.size() = [ batch, channel,x, y]\n",
        "    optimizer.zero_grad() # init grad\n",
        "    pred = model(x) # forward\n",
        "    loss = criterion(perd,x_labels) # calculate loss\n",
        "    loss.backward() # backpropagation\n",
        "    optimizer.step() # weight update\n",
        "    running_loss += loss.item() # 학습과정 출력\n",
        "    if (i+1)%2000 == 0:\n",
        "      print(\"epoch: {}/{} | step: {}/{} | loss: {:.4f}\".format(epoch,num_epochs, i+1, num_batches, running_loss/2000))\n",
        "      running_loss = 0.0\n",
        "\n",
        "print(\"finish Training!\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6WoKw346oydM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}